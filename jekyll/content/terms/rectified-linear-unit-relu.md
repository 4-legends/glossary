---
title: Rectified Linear Unit (ReLU)
related_terms:
 - activation-function
 - neural-network
---
A Rectified Linear Unit is a common name for a neuron (the "unit")
with an activation function of $f(x) = \max(0,x)$.
